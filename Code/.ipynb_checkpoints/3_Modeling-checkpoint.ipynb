{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Notebook\n",
    "## Notebook Goal\n",
    "With all the data prepared and ready to go, it's time to start modelling. This means I'm going to have to vectorize my data in some way, grid search through some models as well as through their parameters, and establish some final predictive model. The modelling parameters are going to be heavily focused on preventing overfitness so that it can generalize well. That will be the biggest detractor here. My model might be overly specific and really good at predicting documents that are very similar to the one it was trained on. But not so great at generalizing it to data it hasn't seen before. Another way to say it would be that an overly fit model loses some of its ability to capture negativity because it's giving more weight to certain non-negative words than it should.\n",
    "\n",
    "## WorkFlow \n",
    "\n",
    "**1)** Create a Baseline model to compare our models to. <br>\n",
    "**2)** Separate space for each of the four training DataSets. <br> \n",
    "**3)** Transform each dataset using either CountVectorizer or Tfidf. <br>\n",
    "**4)** Try the following models on the data: Logistic Regression, Naive Bayes, and a Recurrent Neural Network. <br>\n",
    "**5)** Choose the best model from each data set and create a combined prediction from all of them. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # shhhhhhhh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to define some modelling functions that will help make the process of grid searching smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Useful_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the necessary data first the training data\n",
    "Emotion_short = pd.read_csv('../data/Training_Data/2_Cleaned_Training_Data/Cleaned_Emotion_Analyzer.csv')\n",
    "Emotion_long = pd.read_csv('../data/Training_Data/2_Cleaned_Training_Data/Other_Cleaned_Emotion_Analyzer.csv')\n",
    "Pos_neg = pd.read_csv('../data/Training_Data/2_Cleaned_Training_Data/Cleaned_Pos_Neg_Sentences.csv')\n",
    "Word_Classifier = pd.read_csv('../data/Training_Data/1_Uncleaned_Training_Data/Andbrain_DataSet.csv')\n",
    "# Now the testing data\n",
    "Tester= pd.read_csv('../data/Testing_Data/4_Cleaned_Testing_Data/Final_Testing_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'll do next might seem a little confusing but the purpose of it is to get my data to fit into the parameters of some of the later models. I'm going to change the value `0` into `-1` for the target column in each of the data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zero_to_neg_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f86343605855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEmotion_short\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Negativity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmotion_short\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Negativity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero_to_neg_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mEmotion_long\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Negativity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmotion_long\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Negativity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero_to_neg_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mPos_neg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Negativity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPos_neg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Negativity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero_to_neg_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'zero_to_neg_one' is not defined"
     ]
    }
   ],
   "source": [
    "Emotion_short['Negativity'] = Emotion_short['Negativity'].apply(zero_to_neg_one)\n",
    "Emotion_long['Negativity'] = Emotion_long['Negativity'].apply(zero_to_neg_one)\n",
    "Pos_neg['Negativity'] = Pos_neg['Negativity'].apply(zero_to_neg_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Grid Search using Logistic Regression\n",
    "# def log_cvec(pipe_param, X_train, y_train, X_test, y_test):\n",
    "#     pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "#                      ('lr', LogisticRegression())])\n",
    "\n",
    "#     gs = GridSearchCV(pipe, param_grid=pipe_param, cv=5)\n",
    "#     gs.fit(X_train, y_train)\n",
    "#     train_predictions = gs.predict(X_train)\n",
    "#     test_predictions = gs.predict(X_test)\n",
    "#     print(f'The best score was: {gs.best_score_}')\n",
    "#     print(f'The accuracy score for your training data was: {accuracy_score(train_predictions, y_train)}')\n",
    "#     print(f'The accuracy score for your testing data was: {accuracy_score(test_predictions, y_test)}')\n",
    "#     print(f'The best parameters were: {gs.best_params_}')\n",
    "#     return(pd.DataFrame(confusion_matrix(y_test, test_predictions), \n",
    "#                                          index=['Actual Rational', 'Actual Irrational'], \n",
    "#                                          columns=['Predicted Rational', 'Predicted Irrational']))\n",
    "\n",
    "# # Grid search using Naive Bayes\n",
    "# def nae_vec(pipe_param):\n",
    "#     pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "#                      ('nb', MultinomialNB())])\n",
    "\n",
    "#     gs = GridSearchCV(pipe, param_grid=pipe_param, cv=5)\n",
    "#     gs.fit(X_train, y_train)\n",
    "#     train_predictions = gs.predict(X_train)\n",
    "#     test_predictions = gs.predict(X_test)\n",
    "#     print(f'The best score for the grid search was: {gs.best_score_}')\n",
    "#     print(f'The accuracy score for your training data was: {accuracy_score(train_predictions, y_train)}')\n",
    "#     print(f'The accuracy score for your testing data was: {accuracy_score(test_predictions, y_test)}')\n",
    "#     print(f'The best parameters were: {gs.best_params_}')\n",
    "#     return(pd.DataFrame(confusion_matrix(y_test, test_predictions), \n",
    "#                                          index=['Actual Rational', 'Actual Irrational'], \n",
    "#                                          columns=['Predicted Rational', 'Predicted Irrational']))\n",
    "\n",
    "# def rnn_cvec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Irrational</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh of course</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lately i ve been having these attack that are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>well it becomes a total preoccupation i can t ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>patrick that s my husband he wa late he lost h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>well somehow i finally got myself together and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Text  Irrational\n",
       "0           0                                       oh of course           0\n",
       "1           1  lately i ve been having these attack that are ...           0\n",
       "2           2  well it becomes a total preoccupation i can t ...           1\n",
       "3           3  patrick that s my husband he wa late he lost h...           1\n",
       "4           4  well somehow i finally got myself together and...           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    259\n",
       "0    223\n",
       "Name: Irrational, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester.Irrational.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is pretty balanced which is important when we attempt to predict. Also any model would have to beat a base line 259/482 or 53.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Tester['Text']\n",
    "y_test = Tester['Irrational']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Emotion Data\n",
    "I called it \"short\" because in the original emotion listing there was only 6 emotions compared to the other emotion classifier with 12 emotions. Let's do some quick summary stats to get a feel for what will happen in modelling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Negativity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          Sentences  Negativity\n",
       "0           0      i just feel really helpless and heavy hearted           1\n",
       "1           1  ive enjoyed being able to slouch about relax a...           1\n",
       "2           2  i gave up my internship with the dmrg and am f...           1\n",
       "3           3                         i dont know i feel so lost           1\n",
       "4           4  i am a kindergarten teacher and i am thoroughl...           1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotion_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotion_short.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotion_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5451\n",
       "0    4549\n",
       "Name: Negativity, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotion_short.Negativity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there's a decent amount of data here and a good amount of examples from both classes. So let's move on to modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorized Short Emotion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(482,)\n",
      "(10000,)\n",
      "(482,)\n",
      "The best score was: 0.7715\n",
      "The accuracy score for your training data was: 0.8933\n",
      "The accuracy score for your testing data was: 0.5394190871369294\n",
      "The best parameters were: {'cvec__max_df': 0.999, 'cvec__min_df': 0, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': 'english', 'lr__C': 0.01, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "X_train = Emotion_short['Sentences']\n",
    "y_train = Emotion_short['Negativity']\n",
    "pipe_param =  { 'cvec__stop_words':['english'],\n",
    "                'cvec__min_df': [0, .001],\n",
    "                'cvec__max_df': [.999],\n",
    "                'cvec__ngram_range': [(1, 3),(1, 6)],\n",
    "                'lr__C': [.01],\n",
    "                'lr__penalty': ['l2']}\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('lr', LogisticRegression())])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_param, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "train_predictions = gs.predict(X_train)\n",
    "test_predictions = gs.predict(X_test)\n",
    "print (train_predictions.shape)\n",
    "print(test_predictions.shape)\n",
    "print (y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(f'The best score was: {gs.best_score_}')\n",
    "print(f'The accuracy score for your training data was: {accuracy_score(train_predictions, y_train)}')\n",
    "print(f'The accuracy score for your testing data was: {accuracy_score(test_predictions, y_test)}')\n",
    "print(f'The best parameters were: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Rational</th>\n",
       "      <th>Predicted Irrational</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Rational</th>\n",
       "      <td>22</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Irrational</th>\n",
       "      <td>21</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Rational  Predicted Irrational\n",
       "Actual Rational                    22                   201\n",
       "Actual Irrational                  21                   238"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(pd.Series(y_test), pd.Series(test_predictions)),\n",
    "                                     index=['Actual Rational', 'Actual Irrational'], \n",
    "                                     columns=['Predicted Rational', 'Predicted Irrational'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Emotion_short['Sentences']\n",
    "y_train = Emotion_short['Negativity']\n",
    "pipe_param =  { 'cvec__stop_words':['english'],\n",
    "                'cvec__min_df': [0, .001],\n",
    "                'cvec__max_df': [.999],\n",
    "                'cvec__ngram_range': [(1, 3),(1, 6)],\n",
    "                'lr__C': [.01],\n",
    "                'lr__penalty': ['l2']}\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('lr', LogisticRegression())])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_param, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "train_predictions = gs.predict(X_train)\n",
    "test_predictions = gs.predict(X_test)\n",
    "print (train_predictions.shape)\n",
    "print(test_predictions.shape)\n",
    "print (y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(f'The best score was: {gs.best_score_}')\n",
    "print(f'The accuracy score for your training data was: {accuracy_score(train_predictions, y_train)}')\n",
    "print(f'The accuracy score for your testing data was: {accuracy_score(test_predictions, y_test)}')\n",
    "print(f'The best parameters were: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Emotion_short['Sentences']\n",
    "y_train = Emotion_short['Negativity']\n",
    "pipe_param =  { 'cvec__stop_words':['english'],\n",
    "                'cvec__min_df': [0, .001],\n",
    "                'cvec__max_df': [.999],\n",
    "                'cvec__ngram_range': [(1, 3),(1, 6)],\n",
    "                'lr__C': [.01],\n",
    "                'lr__penalty': ['l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cvec(pipe_param, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_param =  {'cvec__stop_words':['english'],\n",
    "                'cvec__min_df': [0],\n",
    "                'cvec__max_df': [.98],\n",
    "                'cvec__ngram_range': [(1,15),(1,10)],\n",
    "              'nb__alpha': [2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nae_vec(pipe_param, X_train, y_train, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer Short Emotion Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorized Long Emotion Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer Long Emotion Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorized Positive and Negative Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer Positive and Negative Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
